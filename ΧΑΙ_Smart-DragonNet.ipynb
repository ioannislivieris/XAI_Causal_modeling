{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "\n",
    "# Basic libraries\n",
    "#\n",
    "import random\n",
    "import time\n",
    "import numpy  as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "# Sklearn library\n",
    "#\n",
    "from sklearn.preprocessing   import StandardScaler\n",
    "\n",
    "\n",
    "# =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "# Tensorflow library\n",
    "#\n",
    "import tensorflow                as tf\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks  import TerminateOnNaN, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.utils      import plot_model\n",
    "\n",
    "\n",
    "# =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "# User libraries\n",
    "#\n",
    "from utils.metrics      import PEHE, ATE\n",
    "from utils.Loss         import *\n",
    "from utils.DragonNet    import *\n",
    "from utils.utils        import MSE\n",
    "from utils.dataloader   import *\n",
    "\n",
    "print('[INFO] All libraries were imported')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Problem = 'Synthetic'\n",
    "problem_type = 'linear'  # 'linear', 'sin'\n",
    "train_size = 0.8\n",
    "n_f, n_i = 10, 5\n",
    "nInstances = 1000 # size\n",
    "p = 0.3\n",
    "seed = 42\n",
    "\n",
    "Model = 'Smart-DragonNet'\n",
    "\n",
    "\n",
    "\n",
    "random.seed(seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "np.random.seed(seed)\n",
    "# tf.random.set_seed(seed) # for Tensoflow >= 2.0\n",
    "tf.random.set_random_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targeted_regularization    = True # {True, False}\n",
    "\n",
    "output_dir                 = ''\n",
    "knob_loss                  = dragonnet_loss_binarycross\n",
    "ratio                      = 1.\n",
    "validation_split           = 0.2\n",
    "batch_size                 = 64\n",
    "verbose                    = False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "metrics = [regression_loss, binary_classification_loss, treatment_accuracy, track_epsilon]\n",
    "\n",
    "if targeted_regularization:\n",
    "    loss = make_tarreg_loss(ratio=ratio, dragonnet_loss=knob_loss)\n",
    "else:\n",
    "    loss = knob_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataLoader = Synthetic(type=problem_type, size=nInstances, n_f=n_f, n_i=n_i, p=p, seed=seed)\n",
    "DataLoader.create_dataset(train_size=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "# Simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Start timer\n",
    "#\n",
    "start1 = time.time()\n",
    "\n",
    "\n",
    "# Load training data\n",
    "#\n",
    "trainX, trainT, trainY, train_potential_Y = DataLoader.get_training_data()\n",
    "\n",
    "# Load testing data\n",
    "#\n",
    "testX, testT, testY, test_potential_Y = DataLoader.get_testing_data()\n",
    "#\n",
    "print('[INFO] Datasets imported')\n",
    "\n",
    "# Remove irrelevant features\n",
    "trainX = trainX[:, n_i:]\n",
    "testX = testX[:, n_i:]\n",
    "\n",
    "# Setup scaler for inputs\n",
    "scalerX = StandardScaler()\n",
    "trainX  = scalerX.fit_transform( trainX )\n",
    "testX   = scalerX.transform( testX )\n",
    "\n",
    "\n",
    "# Setup scaler for target variable\n",
    "#\n",
    "scalerY       = StandardScaler()\n",
    "trainY_scaled = scalerY.fit_transform( trainY.reshape(-1,1) ).squeeze(-1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Setup DragonNet\n",
    "#\n",
    "dragonnet = make_dragonnet(trainX.shape[1], 0.01)\n",
    "\n",
    "\n",
    "# Create outputs for DragonNet (concatenate Y & T)\n",
    "#\n",
    "yt_train = np.concatenate([trainY_scaled.reshape(-1,1), trainT.reshape(-1,1)], axis = 1)\n",
    "\n",
    "\n",
    "#\n",
    "#\n",
    "# *** Training - Phase I ***\n",
    "#\n",
    "#\n",
    "\n",
    "# Compile network\n",
    "#\n",
    "dragonnet.compile(optimizer = Adam(lr=1e-3), \n",
    "                    loss      = loss, \n",
    "                    metrics   = metrics)\n",
    "\n",
    "# Setup callbacks\n",
    "callbacks = [TerminateOnNaN(),\n",
    "                EarlyStopping(monitor   = 'val_loss', \n",
    "                            patience  = 2, \n",
    "                            min_delta = 0.),\n",
    "                ReduceLROnPlateau(monitor   = 'loss', \n",
    "                                factor    = 0.5, \n",
    "                                patience  = 5, \n",
    "                                verbose   = verbose, \n",
    "                                mode      = 'auto', \n",
    "                                min_delta = 1e-8, \n",
    "                                cooldown  = 0, \n",
    "                                min_lr    = 0)]\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Training\n",
    "#\n",
    "dragonnet.fit(trainX, yt_train, \n",
    "                callbacks        = callbacks,\n",
    "                validation_split = validation_split,\n",
    "                epochs           = 100,\n",
    "                batch_size       = batch_size, \n",
    "                verbose          = verbose)\n",
    "\n",
    "\n",
    "print(\"[INFO] Training - Phase I - Time %.2f secs\" % (time.time() - start_time) )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#\n",
    "#\n",
    "# *** Training - Phase II ***\n",
    "#\n",
    "#    \n",
    "\n",
    "# Setup callbacks\n",
    "#\n",
    "callbacks = [TerminateOnNaN(),\n",
    "                EarlyStopping(monitor   = 'val_loss', \n",
    "                            patience  = 40, \n",
    "                            min_delta = 0.),\n",
    "                ReduceLROnPlateau(monitor   = 'loss', \n",
    "                                factor    = 0.5, \n",
    "                                patience  = 5, \n",
    "                                verbose   = verbose, \n",
    "                                mode      = 'auto',\n",
    "                                min_delta = 0., \n",
    "                                cooldown  = 0, \n",
    "                                min_lr    = 0)\n",
    "]\n",
    "\n",
    "# Compile network\n",
    "#\n",
    "dragonnet.compile(optimizer = SGD(lr=1e-5, momentum=0.9, nesterov=True), \n",
    "                    loss      = loss,\n",
    "                    metrics   = metrics)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Training\n",
    "#\n",
    "dragonnet.fit(trainX, yt_train, \n",
    "                callbacks        = callbacks,\n",
    "                validation_split = validation_split,\n",
    "                epochs           = 300,\n",
    "                batch_size       = batch_size, \n",
    "                verbose          = verbose)\n",
    "\n",
    "print(\"[INFO] Training - Phase II - Time %.2f secs\" % (time.time() - start_time) )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#\n",
    "#\n",
    "# *** Predictions ***\n",
    "#\n",
    "#       \n",
    "yt_hat_test  = dragonnet.predict( testX )\n",
    "\n",
    "# Get predictions\n",
    "#\n",
    "test_y_hat = yt_hat_test[:,:2]\n",
    "\n",
    "# Apply inverse transformation\n",
    "#\n",
    "test_y_hat[:,0] = scalerY.inverse_transform( test_y_hat[:,0].reshape(-1,1) )[0]\n",
    "test_y_hat[:,1] = scalerY.inverse_transform( test_y_hat[:,1].reshape(-1,1) )[0]\n",
    "\n",
    "# Get propensity score\n",
    "#\n",
    "propensity_score = yt_hat_test[:,2]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Results\n",
    "Results = dict()\n",
    "Results['Error_PEHE'] = PEHE(test_potential_Y, test_y_hat)\n",
    "Results['Error_ATE'] = ATE(test_potential_Y, test_y_hat)  \n",
    "Results['MSE_0'] = MSE(test_potential_Y[:,0], test_y_hat[:,0])\n",
    "Results['MSE_1'] = MSE(test_potential_Y[:,1], test_y_hat[:,1])\n",
    "\n",
    "    \n",
    "print('[INFO] Error of PEHE and ATE computed')\n",
    "print('[INFO] Time %.2f\\n\\n' % (time.time() - start1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "filename = f'Results/Problem={Problem}-{problem_type}-{p}_Model={Model}.json'\n",
    "\n",
    "with open(filename, \"w\") as outfile:\n",
    "    json.dump(Results, outfile)\n",
    "\n",
    "Results\n",
    "\n",
    "# {'Error_PEHE': 1772.8156294658145,\n",
    "#  'Error_ATE': 41.191034161971594,\n",
    "#  'MSE_0': 322100.54965537897,\n",
    "#  'MSE_1': 281469.1645955287}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permutation Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set feature names\n",
    "Features = [f'X{i+1}' for i in range(testX.shape[1])]\n",
    "\n",
    "# Number of Iteration for Permutation-Feature-Importance\n",
    "nSimulations = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "Performance = {'Error_PEHE': [], 'Error_ATE': [], 'MSE_0': [], 'MSE_1':  [], 'Feature': [], 'Iteration': []}\n",
    "\n",
    "# Features\n",
    "# =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "for i in range(testX.shape[1]):\n",
    "    print('Feature: ', Features[i])\n",
    "\n",
    "    for Sim in tqdm(range(nSimulations)):\n",
    "        testX_new = testX.copy()\n",
    "        \n",
    "        # Shuffle Feature-i\n",
    "        np.random.shuffle(testX_new[:,0])\n",
    "\n",
    "        # Get predictions\n",
    "        yt_hat_test  = dragonnet.predict( testX_new )\n",
    "        test_y_hat = yt_hat_test[:,:2]\n",
    "\n",
    "        # Apply inverse transformation\n",
    "        test_y_hat[:,0] = scalerY.inverse_transform( test_y_hat[:,0].reshape(-1,1) )[0]\n",
    "        test_y_hat[:,1] = scalerY.inverse_transform( test_y_hat[:,1].reshape(-1,1) )[0]\n",
    "\n",
    "        # Include Feature performance\n",
    "        Performance['Error_PEHE'] += [PEHE(test_potential_Y, test_y_hat)]\n",
    "        Performance['Error_ATE'] += [ATE(test_potential_Y, test_y_hat)  ]\n",
    "        Performance['MSE_0'] += [MSE(test_potential_Y[:,0], test_y_hat[:,0])]\n",
    "        Performance['MSE_1'] += [MSE(test_potential_Y[:,1], test_y_hat[:,1])]\n",
    "        Performance['Feature'] += [Features[i]]\n",
    "        Performance['Iteration'] += [Sim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(Performance)\n",
    "\n",
    "for performanceMetric in ['Error_PEHE', 'Error_ATE', 'MSE_0', 'MSE_1']:\n",
    "    df[performanceMetric] = df[performanceMetric] - Results[performanceMetric]\n",
    "\n",
    "\n",
    "df.to_csv(f'PFI/Problem={Problem}-{problem_type}-{p}_Model={Model}.csv', index = False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "explainer = shap.DeepExplainer(dragonnet, trainX)\n",
    "\n",
    "shap_values = explainer.shap_values(testX)\n",
    "\n",
    "np.savez(f'SHAP/Problem={Problem}-{problem_type}-{p}_Model={Model}.npz', shap_values=shap_values, Features=np.array(Features))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "307.2px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "f8692f46610980770f61291bec1cab73f5a660600a4995e682050817c9e58235"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
